<?xml version='1.0' encoding='utf-8'?>
<doc><id>2018063736387</id><url>https://www.3dmgame.com/news/201806/3736387.html</url><title>谷歌发布AI开发和使用指南：禁止研发杀人武器</title><datetime>2018-06-08 20:25:5</datetime><text>继停止了与军方的合作后，谷歌今天公布了AI开发和使用指南，禁止制造武器在内的可能危害人类的AI应用。
据VentureBeat报道，谷歌今天发布了人工智能（AI）开发和使用指南，其中包括禁止制造自动武器和大多数可能危害人类的AI应用。AI指南发布几天前，谷歌宣布不再与美国国防部续签分析无人机视频的合同。
谷歌将自己定位为“AI优先”科技公司、流行开源框架（如Kaggle和TensorFlow）的所有者以及知名研究人员的雇主，它也是AI领域最有影响力的公司之一。谷歌首席执行官桑达尔·皮查伊（Sundar  Pichai）：“我们想要澄清的是，虽然我们不开发用于武器的AI，但我们将继续在其他领域与政府和军方合作，包括网络安全、培训、军事招募、退伍军人医疗、搜索和救援等。”

在博文中，皮查伊阐述了开发和使用AI时应考虑的原则，以及谷歌不会追求的AI应用类别。除了禁止设计自主武器之外，该公司还将寻求避免研发可能伤害或以伤害人类为主要目的的AI，以及“反国际公认规范的收集或使用信息进行监视的技术”。皮查伊表示，谷歌公司研发的AI应该坚持科学卓越的标准，对人类负责，测试后确认安全，以及避免强化偏见。
此前，Google AI主管杰夫·迪安(Jeff  Dean)和3000多名员工签署了一份请愿书，敦促谷歌承诺绝不制造自动武器。据报道，约有12名员工因谷歌公司参与国防部的AI项目Project  Maven而辞职。该请愿书敦促谷歌退出该项目，在谷歌“正努力维护公众的信任”之际，不这么做可能“无可挽回地损害谷歌的品牌和其争夺人才的能力”。
意识到潜在的争议后，早在去年秋天，Google Cloud首席科学家李飞飞博士就敦促公司“不惜一切代价”避免提及AI，因为当公众提及Project  Maven时，称其为“AI武器化”，这可能最敏感的AI话题之一。
皮查伊坚持谷歌将继续与军方合作，这可能是一个信号，表明谷歌仍计划争夺联合企业防御基础设施(JEDI)合同。这是一项与美国军方签订的为期10年、价值100亿美元的云协议，亚马逊(Amazon)和谷歌等大型科技公司都非常关注。
一份内部备忘录还透露，谷歌的国防销售团队成员认为，参Project  Maven与一份价值数十亿美元的政府合同直接相关。据报道，谷歌认为其与五角大楼签署的合同每年可带来高达2.5亿美元的收入，尽管员工们最初被告知合同价值仅为900万美元。
三月份有消息称谷歌参与了Project Maven。其他公司，如IBM，也被邀请参与该项目。上周，五角大楼宣布了扩展Project  Maven的计划，并开设了联合人工智能中心。谷歌并不是唯一一家因为参与研发自主武器而受到员工反对的企业。今年4月份，超过50名韩国研究人员抵制KAIST人工智能武器实验室成立。
皮查伊博客全文：
AI的核心是学习和适应计算机编程，它不能解决所有问题，但它改善我们生活的潜力是深远的。在谷歌，我们使用AI让产品变得更有用——从更容易撰写的无垃圾邮件到可与你自然交谈的数字助理，再到让你尽情享受的照片应用。
在我们的产品之外，我们正在使用AI帮助人们解决紧迫问题。两名高中生正在研发AI传感器来预测火灾的风险，农民们正在用AI来监控他们的牲畜健康状况，医生们开始使用AI来帮助诊断癌症和预防失明。这些明显的好处就是为何谷歌在AI研发上投入大量资金，并通过我们的工具和开源代码使AI技术获得广泛应用的原因。
我们认识到，如此强大的技术对其如何被使用也提出了同样巨大的挑战。在未来几年，AI的开发和使用方式将对社会产生重大影响。作为AI领域的领导者，我们有责任把这一切做好。所以今天，我们宣布七大原则来指导我们的工作。这些都不是理论概念，它们是具体的标准，将积极地指导我们的研究和产品开发，并将影响我们的业务决策。
我们承认这一领域处于动态和不断发展状态中，我们将以谦逊的态度对待我们的工作，致力于内部和外部接触，并愿意随时间的推移而调整我们的方法。
AI应用目标
我们将根据以下目标评估AI应用程序，我们认为AI应该：
1.有利于社会
新技术的广泛应用日益影响到整个社会。AI的进步将在医疗、安全、能源、交通、制造和娱乐等广泛领域产生革命性影响。在我们考虑AI技术的潜在开发和应用时，我们将考虑广泛的社会和经济因素，并将在我们认为总体收益大大超过可预见的风险和不利因素的情况下进行。AI也大幅增强了我们理解内容意义的能力。我们将努力利用AI随时提供高质量和准确的信息，同时继续尊重我们所在国家的文化、社会和法律规范。我们将继续深入地评估：何时使我们的技术在非商业基础中可用。
2.避免产生或加强不公平的偏见
AI算法和数据集可以反映、增强或减少不公平的偏差。我们认识到区分公平与不公平偏见并不总是简单的，在不同的文化和社会中，公平的定义也存在差异。我们将设法避免AI对人类造成不公正的影响，特别是那些与敏感特征有关的影响，如种族、种族、性别、国籍、收入、性取向、能力、政治或宗教信仰。
3.构建并测试AI安全性
我们将继续制定和实施强有力的安全措施，以避免AI造成意外伤害及其后果。我们将设计保持谨慎的AI系统，并根据AI安全研究的最佳实践进行开发。在适当的情况下，我们将在受限环境中测试AI技术，并在部署后监视它们的操作。
4.对人类负责
我们将设计AI系统，它可为反馈、相关解释和申诉提供适当的机会。我们的AI技术将受到适当的人类指导和控制。
5.符合隐私设计原则
我们将把我们的隐私原则纳入AI技术的开发和使用中。我们将给予通知和同意的机会，鼓励具有隐私保护的架构，并提供适当的透明度和对数据使用的控制。
6.坚持科学卓越的标准
技术创新根植于科学方法，并需要开放性探究、保持智力严谨、诚信和协作。AI工具有可能在生物学、化学、医学和环境科学等关键领域开启新的科学研究和知识领域。在我们推动AI发展的过程中，我们渴望达到高水平的科学成就。我们将与一系列利益相关者共同努力，在这一领域推进先进的思想，并采用科学严谨的方法。我们将以负责任的方式分享AI知识，出版教育材料、最佳实践和研究，使更多的人能够开发有用的AI应用程序。
7.开发符合上述原则的AI用例
许多技术都有多种用途。我们将努力限制潜在有害或滥用的AI技术。随着我们开发和部署AI技术，我们将根据以下因素评估可能的用途：
1）主要目的和用途：技术和应用的主要目的和可能用途，包括解决方案与有害使用的关系或适应性?
2）性质和独特性：我们是否提供独特的或更普遍的技术?
3）规模：该技术的使用是否会产生重大影响
4）谷歌参与的性质：我们是否提供通用工具，为客户集成工具，还是开发自定义解决方案
谷歌不会追求的AI应用
除了上述目标，我们不会在以下应用领域设计或部署AI：
1）造成或可能造成总体伤害的技术。在有重大风险的领域，我们只会在我们认为好处远远大于风险的部分进行，并将纳入适当的安全限制;
2）武器或其他技术，其主要目的或实施造成或直接导致人类受到伤害;
3）收集或使用信息进行监视的技术，它们违反国际公认规范;
4）其目的违反普遍接受的国际法和人权原则的AI技术。
我们要澄清的是，虽然我们没有开发用于武器的AI，但我们将继续在其他许多领域与政府和军方合作，其中包括网络安全、培训、军事招募、退伍军人医疗、搜索和救援。这些合作非常重要，我们将积极寻找更多的方法来加强这些组织的关键工作，并确保服务人员和平民的安全。
AI远景
虽然这是我们选择与AI打交道的方式，但我们明白，在这场对话中，需要更多其他声音加入。随着AI技术的发展，我们将与一系列利益相关方合作，在这一领域推广更先进的思想，采用科学严谨、多学科的方法。我们将继续分享我们在改进AI技术和实践方面的经验。
我们相信这些原则是我们公司和AI未来发展的正确基础。这种做法与2004年“创始人的信”（Founders’  Letter）中提出的价值观相一致。当时我们曾明确表示，我们打算从长远的角度出发，即使这意味着要做出短期的权衡。
</text><imageurl>https://img.3dmgame.com/uploads/allimg/180608/369_180608202455_1_lit.jpeg	</imageurl><videourl /></doc>